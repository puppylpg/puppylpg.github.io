---
layout: post
title: "Elasticsearch：数据重分配"
date: 2023-02-09 22:47:54 +0800
categories: elasticsearch rehash
tags: elasticsearch rehash
---

之前看elasticsearch按照`_routing`的hash对文档进行分片的时候，竟然都没有注意到elasticsearch实现做虚拟分片，再映射到实体分片……
```
routing_factor = num_routing_shards / num_primary_shards
shard_num = (hash(_routing) % num_routing_shards) / routing_factor
```
`num_routing_shards`就是虚拟分片的个数。

为什么要这么搞？因为分布式系统涉及到节点变动时数据重新分配的问题。

1. Table of Contents, ordered
{:toc}

# 数据重分配
大前提：分布式系统，每一个服务节点都只能有部分数据。当节点增加或减少的时候，需要重新分配数据。如果分配数据到节点只是单纯地使用了hash算法，那么就需要rehash。

## rehash
方案最简单，但是每当增减一个节点，几乎所有的key都要rehash到不同的节点，对服务来说变动太大，要移动的数据太多。

## 一致性哈希 Consistent Hashing
- https://bbs.huaweicloud.com/blogs/333158

本质上是**新增一个节点之后，只去分担前一个节点的部分key，所以完全不影响其他节点上的key，要移动的数据范围很小**。

**但是这样很容易导致数据分布不均匀，所以一致性哈希一般使用大量的虚拟节点**，把数据切分的比较细，再把多个虚拟节点映射到物理节点上，就均匀了。**如果新增一个物理节点，就会相应增加一些虚拟节点，抢过来一些其他虚拟节点的数据，可以理解为是均匀抢的其他节点。如果下掉一个物理节点，就下掉它对应的虚拟节点，把这些虚拟节点的数据都分配给环上的下一个虚拟节点，对其他物理机来说，基本也是均匀增加数据**。

## 一分为二的hash
在JDK的HashMap里，扩容是这么玩的：每次容量扩大一倍，就可以做到把一个桶只拆为对应的两个桶。

- https://tech.meituan.com/2016/06/24/java-hashmap.html

**elasticsearch分片拆分的原理跟HashMap几乎一样**。当elasticsearch的分片太大的时候，可以增加分片数，使用[split API](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-split-index.html)将大分片拆分为体积较小的分片。

再看es的[`_routing`](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-routing-field.html)，虽然是hash取模分片，但在按照实际分片数取模之前，先按照虚拟分片数取模：
```
routing_factor = num_routing_shards / num_primary_shards
shard_num = (hash(_routing) % num_routing_shards) / routing_factor
```

虚拟分片数是通过[`index.number_of_routing_shards`](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html#index-number-of-routing-shards)设置的，**默认值是主分片数的2^n，同时不超过1024**。比如primary shard=30，虚拟分片数就是30x2^5=960。

拆分索引的时候就可以按照主分片的2^n拆分，比如一拆二就是设置新索引的主分片数为30x2=60。

### elasticsearch的考量
为什么elasticsearch不采用普通的rehash？rehash代价太大，对key value系统如此，对es就更不用说了。

[为什么不采用一致性哈希](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-split-index.html#incremental-resharding)？**虽然只需要挪n分之一的数据，es仍然认为代价太大**。因为相比简单的key value系统，es的每个文档都要在创建时建立索引。**和索引新文档比起来，还是删数据的速度更快——**

> The most common way that key-value stores do this efficiently is by using consistent hashing. Consistent hashing only requires 1/N-th of the keys to be relocated when growing the number of shards from N to N+1. However Elasticsearch’s unit of storage, shards, are Lucene indices. Because of their search-oriented data structure, taking a significant portion of a Lucene index, be it only 5% of documents, deleting them and indexing them on another shard typically comes with a much higher cost than with a key-value store.

为什么一拆2/4/8/...就可以？因为es可以直接给新索引创建硬链接指向之前的数据，然后删掉一半不再属于该分片的数据即可。比重新索引一个个文档简单。

> The split is done efficiently by hard-linking the data in the source primary shard into multiple primary shards in the new index, **then running a fast Lucene Delete-By-Query to mark documents which should belong to a different shard as deleted**. These deleted documents will be physically removed over time by the background merge process.

- https://www.elastic.co/cn/blog/elasticsearch-6-1-0-released

**即使理解成把原索引copy成两个也是可以的：**

> If the file system doesn't support hard-linking, then all segments are copied into the new index, which is a much more time consuming process.

**之后删掉一半的文档，一定比重新索引分片上的全部文档要高效得多。这一点是elasticsearch根据自己的系统特性选用的和HashMap不一样的挪动思路**。hashmap之所以直接挪了，毕竟它是内存里的数据，又是单纯的key value数据，挪起来代价也小。

> **即便HashMap在内存里，又是kv数据，挪起来很快，但因为HashMap使用频率非常高，所以也没有选择无脑使用rehash重新分配所有数据，而是一拆二，直接用比原有桶高一位的bit值是否为0或1直接判断的应该属于拆分后的哪个桶**。

Why does elasticsearch still use simple routing value using modulo?
- https://stackoverflow.com/questions/46236029/why-does-elasticsearch-still-use-simple-routing-value-using-modulo

## 手动分配
redis cluster存放数据的基本单位是slot（**很像一致性哈希的虚拟节点，或者elasticsearch的虚拟分片**）。slot数目是固定的，以slot为单位手动指定存放slot的node。需要重新分配的时候直接把整个slot挪过去就行了。

> [Redis - cluster]({% post_url 2021-01-31-redis-cluster %})的slot部分。

redis cluster因为是内存型服务，且存放的是kv数据，挪起来相对还是很快的，所以这里直接挪slot就行了。不像HashMap那样一拆二，更不像elasticsearch那样先复制文件再删除一半。

## 总结
- 普通rehash：最简单，最大的灵活性，最大的数据重分配代价；
- 一致性hash：比较麻烦，灵活性很大，数据重分配代价小；
- 一分为二的hash：
    + HashMap：就是要快；
    + elasticsearch：也是为了快一些，不过也和文件系统相适应；
- 手动分配：redis无所畏惧，反正数据挪起来本身就很快；

# index split流程
A split operation:
1. Creates a new target index with the same definition as the source index, but with a larger number of primary shards.
2. Hard-links segments from the source index into the target index. (If the file system doesn’t support hard-linking, then all segments are copied into the new index, which is a much more time consuming process.)
3. Hashes all documents again, after low level files are created, to delete documents that belong to a different shard.
4. Recovers the target index as though it were a closed index which had just been re-opened.

**首先[设置索引禁止写入](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-blocks.html)**：
```
PUT stored_kol_split_lhb/_settings
{
  "settings": {
    "index.blocks.write": true
  }
}
```
由于原索引主分片为2，这里新索引设置为它的2^5=32倍，64片，所以相当于原来的索引一拆32：
```
POST /stored_kol_split_lhb/_split/stored_kol_split_after_lhb
{
  "settings": {
    "index.number_of_shards": 64
  }
}
```
新的索引把禁止写入的设置也同步了过来，所以搞定后别忘了取消禁止写入：
```
PUT stored_kol_split_after_lhb/_settings
{
  "settings": {
    "index.blocks.write": false
  }
}
```
分片增加之后，虽然文档数没变，但占用空间变大了不少。

如果有某些分片没有成功分配，可以使用[diagnose api](https://www.elastic.co/guide/en/elasticsearch/reference/current/diagnose-unassigned-shards.html)查查原因：
```
GET _cluster/allocation/explain
{
  "index": "stored_kol_split_after_lhb", 
  "shard": 4, 
  "primary": true 
}
```

> 虽然split api需要先设置索引禁止写入，看起来还不如reindex（新建一个索引重新设置主分片数），但是根据上面的介绍，在数据量比较大的时候，**使用splite api直接copy索引的segment文件并删除一半数据，一定比reindex快非常多！**

split index api文档还提到了一句话：如果data是append only，就没必要拆分了。新的数据写到新索引，老索引和新索引设置同样的alias，就可以一起查了。在查询速度表现上和直接查一个主分片数叠加的索引没什么区别：
> In the case of append-only data, it is possible to get more flexibility by creating a new index and pushing new data to it, while adding an alias that covers both the old and the new index for read operations. Assuming that the old and new indices have respectively M and N shards, this has no overhead compared to searching an index that would have M+N shards.

# 结论
又到了那句真理：没有永远的真理，只有合适不合适。大家都基于自己的系统特性，选择了很多不同的hash方案。知识越辩越明，对比起来学习真的是太爽了。

